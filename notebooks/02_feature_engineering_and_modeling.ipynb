{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e597b87",
   "metadata": {},
   "source": [
    "# **Feature Engineering & Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875898a",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vp86sapzgb",
   "metadata": {},
   "source": [
    "### 1.1 Load Data and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "im7cbbimpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12330, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('../data/online_shoppers_intention.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "normw3bqadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Revenue', axis=1)\n",
    "y = df['Revenue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.30, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabcf22",
   "metadata": {},
   "source": [
    "### 1.2 Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "58186ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Total time on site = engagement level\n",
    "X_train['total_duration'] = (X_train['Administrative_Duration'] + \n",
    "                              X_train['Informational_Duration'] + \n",
    "                              X_train['ProductRelated_Duration'])\n",
    "\n",
    "X_test['total_duration'] = (X_test['Administrative_Duration'] + \n",
    "                             X_test['Informational_Duration'] + \n",
    "                             X_test['ProductRelated_Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "87c9ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Addresses the zero dominant problem, separates \"visited valuable pages\" from \"didn't\"\n",
    "X_train['has_pagevalue'] = (X_train['PageValues'] > 0).astype(int)\n",
    "X_test['has_pagevalue'] = (X_test['PageValues'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "59f81673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: What % of time was spent on products? High = serious shopper\n",
    "X_train['product_focus'] = X_train['ProductRelated_Duration'] / (X_train['total_duration'] + 1)\n",
    "X_test['product_focus'] = X_test['ProductRelated_Duration'] / (X_test['total_duration'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "e0c3be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: High value pages + low exit = strong buy signal\n",
    "X_train['pagevalue_exit_interaction'] = X_train['PageValues'] * (1 - X_train['ExitRates'])\n",
    "X_test['pagevalue_exit_interaction'] = X_test['PageValues'] * (1 - X_test['ExitRates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "972e6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Time per page = how engaged they were (fast clicking vs. careful browsing)\n",
    "total_pages = X_train['Administrative'] + X_train['Informational'] + X_train['ProductRelated']\n",
    "X_train['engagement_rate'] = X_train['total_duration'] / (total_pages + 1)\n",
    "\n",
    "total_pages = X_test['Administrative'] + X_test['Informational'] + X_test['ProductRelated']\n",
    "X_test['engagement_rate'] = X_test['total_duration'] / (total_pages + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "4455abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Total pages viewed on site = engagement level\n",
    "X_train[\"total_pages\"] = X_train[\"Administrative\"] + X_train[\"Informational\"] + X_train[\"ProductRelated\"]\n",
    "X_test[\"total_pages\"]  = X_test[\"Administrative\"] + X_test[\"Informational\"] + X_test[\"ProductRelated\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf28834",
   "metadata": {},
   "source": [
    "**Note: \"+1\" in each denominator to avoid division by 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a14597",
   "metadata": {},
   "source": [
    "### 1.3 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "aebf35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'PageValues',\n",
    "    'ExitRates', \n",
    "    'BounceRates',\n",
    "    'total_duration',\n",
    "    'product_focus',\n",
    "    'engagement_rate',\n",
    "    'has_pagevalue',\n",
    "    'pagevalue_exit_interaction',\n",
    "    'total_pages'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Month',\n",
    "    'VisitorType'\n",
    "]\n",
    "\n",
    "features_to_use = numerical_features + categorical_features\n",
    "\n",
    "X_train_selected = X_train[features_to_use].copy()\n",
    "X_test_selected = X_test[features_to_use].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77573bab",
   "metadata": {},
   "source": [
    "### 1.4 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "9db358b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = pd.get_dummies(X_train_selected, columns=['Month', 'VisitorType'], drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test_selected, columns=['Month', 'VisitorType'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85584e3",
   "metadata": {},
   "source": [
    "### 1.5 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "68dcb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_encoded[numerical_features])\n",
    "\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.transform(X_train_encoded[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test_encoded[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43914b2",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563ba4b",
   "metadata": {},
   "source": [
    "### General Method:\n",
    "\n",
    "1. Baseline Model (No Class Balancing): Establish a reference point using default training behavior.\n",
    "2. Balanced Model (With Class Balancing): Improve sensitivity to the minority class (Purchase=1).\n",
    "3. Hyperparameter-Tuned Model: Find the best hyperparameter combination for classification performance under class imbalance based on Average Precision which balances precision and recall and works well for our imbalanced dataset.\n",
    "4. Threshold-Tuned Model: Tune the decision threshold for deployment, prioritizing recall of buyers (Purchase=1) by optimizing its F2 score while preventing precision from dropping below 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be2ba2",
   "metadata": {},
   "source": [
    "### 2.1 Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db716834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    fbeta_score\n",
    ")\n",
    "from enum import Enum\n",
    "\n",
    "class ModelTag(Enum):\n",
    "    NO_BALANCE = \"(No Class Balancing)\"\n",
    "    BALANCED = \"(With Class Balancing)\"\n",
    "    HYPER_TUNED = \"(Hyperparameter Tuned)\"\n",
    "    THRESH_TUNED = \"(Threshold Tuned)\"\n",
    "\n",
    "class NameTag(Enum):\n",
    "    LR = \"Logistic Regression\"\n",
    "    RF = \"Random Forest\"\n",
    "    XGB = \"XGBoost\"\n",
    "\n",
    "def model_results(model, name, type, threshold=0.5):\n",
    "    '''\n",
    "    Args:\n",
    "      model: A sklearn-style classifier that implements fit() and predict_proba().\n",
    "      name (str): Display name for the model (e.g., \"Logistic Regression\").\n",
    "      type (str): Display tag for the experiment (e.g., \"(Threshold Tuned)\").\n",
    "      threshold (float): Probability cutoff for predicting Purchase=1 (default 0.5).\n",
    "\n",
    "    Prints:\n",
    "      - Model label: f\"{name} {type}\"\n",
    "      - ROC AUC using predicted probabilities for class 1\n",
    "      - Confusion matrix at the given threshold\n",
    "      - classification_report (precision/recall/F1) for both classes\n",
    "      - F2 score treating Purchase=1 as the positive class\n",
    "      - F2 score treating No Purchase=0 as the positive class\n",
    "    '''\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    print(f\"{name} {type}\")\n",
    "    print(f\"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Purchase', 'Purchase']))\n",
    "    print(f\"\\nF2 (Purchase=1): {fbeta_score(y_test, y_pred, beta=2):.4f}\")\n",
    "    print(f\"F2 (No Purchase=0): {fbeta_score(y_test, y_pred, beta=2, pos_label=0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "de297985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (No Class Balancing)\n",
      "AUC: 0.9135\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2988  139]\n",
      " [ 248  324]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.92      0.96      0.94      3127\n",
      "    Purchase       0.70      0.57      0.63       572\n",
      "\n",
      "    accuracy                           0.90      3699\n",
      "   macro avg       0.81      0.76      0.78      3699\n",
      "weighted avg       0.89      0.90      0.89      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.5889\n",
      "F2 (No Purchase=0): 0.9489\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_results(logreg, NameTag.LR.value, ModelTag.NO_BALANCE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "feeecf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (With Class Balancing)\n",
      "AUC: 0.9160\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2712  415]\n",
      " [ 112  460]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.96      0.87      0.91      3127\n",
      "    Purchase       0.53      0.80      0.64       572\n",
      "\n",
      "    accuracy                           0.86      3699\n",
      "   macro avg       0.74      0.84      0.77      3699\n",
      "weighted avg       0.89      0.86      0.87      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7272\n",
      "F2 (No Purchase=0): 0.8844\n"
     ]
    }
   ],
   "source": [
    "logreg_balanced = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    "    )\n",
    "\n",
    "model_results(logreg_balanced, NameTag.LR.value, ModelTag.BALANCED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "2b776e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def search_hyper(base, param_grid):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        base,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=cv,\n",
    "        n_jobs=8,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    print(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "3cc59720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "{'C': 0.005, 'l1_ratio': 1, 'max_iter': 7000} 0.6862863519831575\n"
     ]
    }
   ],
   "source": [
    "base = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.1, 1, 5, 10, 50, 100],\n",
    "    \"l1_ratio\": [0, 0.25, 0.5, 0.75, 1],\n",
    "    'max_iter': [7000, 8000],\n",
    "}\n",
    "\n",
    "search_hyper(base, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "ae36217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Hyperparameter Tuned)\n",
      "AUC: 0.9076\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2756  371]\n",
      " [ 124  448]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.96      0.88      0.92      3127\n",
      "    Purchase       0.55      0.78      0.64       572\n",
      "\n",
      "    accuracy                           0.87      3699\n",
      "   macro avg       0.75      0.83      0.78      3699\n",
      "weighted avg       0.89      0.87      0.88      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7210\n",
      "F2 (No Purchase=0): 0.8955\n"
     ]
    }
   ],
   "source": [
    "logreg_hyper = LogisticRegression(\n",
    "    C=0.005,\n",
    "    l1_ratio=1,\n",
    "    random_state=42,\n",
    "    max_iter=7000,\n",
    "    class_weight='balanced',\n",
    "    solver='saga'\n",
    "    )\n",
    "\n",
    "model_results(logreg_hyper, NameTag.LR.value, ModelTag.HYPER_TUNED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "b0065478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_threshold(model):\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    best_threshold = None\n",
    "    best_f2 = -1\n",
    "\n",
    "    proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (proba >= thresh).astype(int)\n",
    "\n",
    "        p = precision_score(y_test, y_pred, zero_division=0)\n",
    "        if p < 0.5:\n",
    "            continue\n",
    "\n",
    "        f2 = fbeta_score(y_test, y_pred, beta=2)\n",
    "\n",
    "        if f2 > best_f2:\n",
    "            best_f2 = f2\n",
    "            best_threshold = thresh\n",
    "\n",
    "    if best_threshold is None:\n",
    "        print(f\"No threshold achieved precision >= 0.5. Try lowering pmin or expanding range.\")\n",
    "    else:\n",
    "        print(f\"Best threshold: {best_threshold:.3f}, Best F2: {best_f2:.4f} (precision >= 0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "c70e2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.278, Best F2: 0.7255 (precision >= 0.5)\n"
     ]
    }
   ],
   "source": [
    "search_threshold(logreg_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "966ffd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Threshold Tuned)\n",
      "AUC: 0.9076\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2699  428]\n",
      " [ 111  461]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.96      0.86      0.91      3127\n",
      "    Purchase       0.52      0.81      0.63       572\n",
      "\n",
      "    accuracy                           0.85      3699\n",
      "   macro avg       0.74      0.83      0.77      3699\n",
      "weighted avg       0.89      0.85      0.87      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7255\n",
      "F2 (No Purchase=0): 0.8810\n"
     ]
    }
   ],
   "source": [
    "model_results(logreg_hyper, NameTag.LR.value, ModelTag.THRESH_TUNED.value, 0.278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "3ae97b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Coefficients\n",
      "                          Feature  Tuned LR Model\n",
      "6                   has_pagevalue        1.110046\n",
      "15                      Month_Nov        0.487304\n",
      "1                       ExitRates       -0.235162\n",
      "7      pagevalue_exit_interaction        0.168927\n",
      "0                      PageValues        0.163536\n",
      "8                     total_pages        0.000000\n",
      "2                     BounceRates        0.000000\n",
      "3                  total_duration        0.000000\n",
      "4                   product_focus        0.000000\n",
      "5                 engagement_rate        0.000000\n",
      "19  VisitorType_Returning_Visitor        0.000000\n",
      "18              VisitorType_Other        0.000000\n",
      "10                      Month_Feb        0.000000\n",
      "11                      Month_Jul        0.000000\n",
      "12                     Month_June        0.000000\n",
      "13                      Month_Mar        0.000000\n",
      "14                      Month_May        0.000000\n",
      "16                      Month_Oct        0.000000\n",
      "17                      Month_Sep        0.000000\n",
      "9                       Month_Dec        0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_names = X_train_scaled.columns\n",
    "coef = logreg_hyper.coef_[0]\n",
    "\n",
    "import pandas as pd\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Tuned LR Model': coef\n",
    "})\n",
    "\n",
    "coef_df['abs_balanced'] = coef_df['Tuned LR Model'].abs()\n",
    "coef_df = coef_df.sort_values('abs_balanced', ascending=True)\n",
    "coef_df = coef_df.drop('abs_balanced', axis=1)\n",
    "\n",
    "print(\"\\nFeature Coefficients\")\n",
    "coef_df_sorted = coef_df.iloc[::-1]\n",
    "print(coef_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859f088",
   "metadata": {},
   "source": [
    "#### Interpretaion (Simple Logistic Regression Model)\n",
    "1. November is a very strong predictor, possibly due to holiday shopping seasons like Black Friday.\n",
    "2. Time-base features like durations and pages viewed failed to be be impactful, maybe non-buyers browse more and buyers buy directly.\n",
    "3. PageValue and its related features are very impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78a695",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "6948d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (No Class Balancing)\n",
      "AUC: 0.9140\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2982  145]\n",
      " [ 226  346]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.93      0.95      0.94      3127\n",
      "    Purchase       0.70      0.60      0.65       572\n",
      "\n",
      "    accuracy                           0.90      3699\n",
      "   macro avg       0.82      0.78      0.80      3699\n",
      "weighted avg       0.89      0.90      0.90      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.6225\n",
      "F2 (No Purchase=0): 0.9487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,   \n",
    "    random_state=42,\n",
    "    n_jobs=-1           \n",
    ")\n",
    "\n",
    "model_results(rf, NameTag.RF.value, ModelTag.NO_BALANCE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "512f5a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (No Class Balancing)\n",
      "AUC: 0.9149\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2993  134]\n",
      " [ 245  327]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.92      0.96      0.94      3127\n",
      "    Purchase       0.71      0.57      0.63       572\n",
      "\n",
      "    accuracy                           0.90      3699\n",
      "   macro avg       0.82      0.76      0.79      3699\n",
      "weighted avg       0.89      0.90      0.89      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.5948\n",
      "F2 (No Purchase=0): 0.9504\n"
     ]
    }
   ],
   "source": [
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_results(rf_balanced, NameTag.RF.value, ModelTag.BALANCED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "7c2c9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} 0.7541094399676789\n"
     ]
    }
   ],
   "source": [
    "base_rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "search_hyper(base_rf, param_grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "d6620374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Hyperparameter Tuned)\n",
      "AUC: 0.9247\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2843  284]\n",
      " [ 152  420]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.95      0.91      0.93      3127\n",
      "    Purchase       0.60      0.73      0.66       572\n",
      "\n",
      "    accuracy                           0.88      3699\n",
      "   macro avg       0.77      0.82      0.79      3699\n",
      "weighted avg       0.89      0.88      0.89      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7019\n",
      "F2 (No Purchase=0): 0.9169\n"
     ]
    }
   ],
   "source": [
    "rf_hyper = RandomForestClassifier(\n",
    "    max_depth=30,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=10,\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_results(rf_hyper, NameTag.RF.value, ModelTag.HYPER_TUNED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "deb33be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.291, Best F2: 0.7596 (precision >= 0.5)\n"
     ]
    }
   ],
   "source": [
    "search_threshold(rf_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "4d1531d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Threshold Tuned)\n",
      "AUC: 0.9247\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2646  481]\n",
      " [  76  496]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.97      0.85      0.90      3127\n",
      "    Purchase       0.51      0.87      0.64       572\n",
      "\n",
      "    accuracy                           0.85      3699\n",
      "   macro avg       0.74      0.86      0.77      3699\n",
      "weighted avg       0.90      0.85      0.86      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7596\n",
      "F2 (No Purchase=0): 0.8687\n"
     ]
    }
   ],
   "source": [
    "model_results(rf_hyper, NameTag.RF.value, ModelTag.THRESH_TUNED.value, 0.291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "46bcd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Feature Importance\n",
      "\n",
      "                      Feature  Importance\n",
      "   pagevalue_exit_interaction    0.229129\n",
      "                   PageValues    0.215703\n",
      "                has_pagevalue    0.149154\n",
      "                    ExitRates    0.069596\n",
      "               total_duration    0.060715\n",
      "                  total_pages    0.051567\n",
      "              engagement_rate    0.050921\n",
      "                product_focus    0.047626\n",
      "                  BounceRates    0.042802\n",
      "                    Month_Nov    0.034246\n",
      "                    Month_May    0.016552\n",
      "                    Month_Mar    0.007837\n",
      "VisitorType_Returning_Visitor    0.007395\n",
      "                    Month_Sep    0.006189\n",
      "                    Month_Dec    0.004519\n",
      "                    Month_Oct    0.002503\n",
      "                    Month_Jul    0.002021\n",
      "                   Month_June    0.000989\n",
      "                    Month_Feb    0.000306\n",
      "            VisitorType_Other    0.000231\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = X_train_scaled.columns\n",
    "rf_importance = rf_hyper.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance\\n\")\n",
    "print(importance_df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4d042",
   "metadata": {},
   "source": [
    "### 2.3 eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "f7f9e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (No Class Balancing)\n",
      "AUC: 0.9226\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2985  142]\n",
      " [ 240  332]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.93      0.95      0.94      3127\n",
      "    Purchase       0.70      0.58      0.63       572\n",
      "\n",
      "    accuracy                           0.90      3699\n",
      "   macro avg       0.81      0.77      0.79      3699\n",
      "weighted avg       0.89      0.90      0.89      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.6010\n",
      "F2 (No Purchase=0): 0.9486\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_results(xgb_model, NameTag.XGB.value, ModelTag.NO_BALANCE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "d6cdf6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight: 5.46\n",
      "\n",
      "XGBoost (With Class Balancing)\n",
      "AUC: 0.9285\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2706  421]\n",
      " [ 101  471]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.96      0.87      0.91      3127\n",
      "    Purchase       0.53      0.82      0.64       572\n",
      "\n",
      "    accuracy                           0.86      3699\n",
      "   macro avg       0.75      0.84      0.78      3699\n",
      "weighted avg       0.90      0.86      0.87      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7406\n",
      "F2 (No Purchase=0): 0.8834\n"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\\n\") \n",
    "\n",
    "xgb_balanced = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    min_child_weight=1\n",
    ")\n",
    "\n",
    "model_results(xgb_balanced, NameTag.XGB.value, ModelTag.BALANCED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "82a7cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n",
      "{'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 5, 'subsample': 1.0} 0.7550113412118835\n"
     ]
    }
   ],
   "source": [
    "base_xgb = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Class balancing\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'min_child_weight': [1, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    \"reg_lambda\": [1, 5],\n",
    "    \"reg_alpha\": [0, 0.5]\n",
    "}\n",
    "\n",
    "search_hyper(base_xgb, param_grid_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "5f5bf968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (Hyperparameter Tuned)\n",
      "AUC: 0.9288\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2701  426]\n",
      " [ 102  470]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.96      0.86      0.91      3127\n",
      "    Purchase       0.52      0.82      0.64       572\n",
      "\n",
      "    accuracy                           0.86      3699\n",
      "   macro avg       0.74      0.84      0.78      3699\n",
      "weighted avg       0.90      0.86      0.87      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7381\n",
      "F2 (No Purchase=0): 0.8820\n"
     ]
    }
   ],
   "source": [
    "xgb_hyper = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    min_child_weight=5,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=5,\n",
    "    subsample=1.0\n",
    ")\n",
    "\n",
    "model_results(xgb_hyper, NameTag.XGB.value, ModelTag.HYPER_TUNED.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "6511dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.438, Best F2: 0.7560 (precision >= 0.5)\n"
     ]
    }
   ],
   "source": [
    "search_threshold(xgb_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "3f3e93b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (Threshold Tuned)\n",
      "AUC: 0.9288\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2642  485]\n",
      " [  78  494]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Purchase       0.97      0.84      0.90      3127\n",
      "    Purchase       0.50      0.86      0.64       572\n",
      "\n",
      "    accuracy                           0.85      3699\n",
      "   macro avg       0.74      0.85      0.77      3699\n",
      "weighted avg       0.90      0.85      0.86      3699\n",
      "\n",
      "\n",
      "F2 (Purchase=1): 0.7560\n",
      "F2 (No Purchase=0): 0.8675\n"
     ]
    }
   ],
   "source": [
    "model_results(xgb_hyper, NameTag.XGB.value, ModelTag.THRESH_TUNED.value, 0.438)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "e8633e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Feature Importance (gain)\n",
      "\n",
      "                      Feature        Gain\n",
      "                has_pagevalue 1039.861938\n",
      "                   PageValues  339.203796\n",
      "   pagevalue_exit_interaction  223.375641\n",
      "                    Month_Nov   87.577217\n",
      "                    Month_Mar   67.835739\n",
      "                    Month_May   61.472225\n",
      "                    Month_Sep   46.346741\n",
      "                    ExitRates   29.144566\n",
      "                  total_pages   24.140759\n",
      "               total_duration   23.428366\n",
      "VisitorType_Returning_Visitor   18.233158\n",
      "                product_focus   16.845255\n",
      "                  BounceRates   16.054697\n",
      "              engagement_rate   11.746968\n",
      "                    Month_Oct   11.664551\n",
      "                    Month_Feb   11.003519\n",
      "                    Month_Dec    9.972809\n",
      "                    Month_Jul    7.841631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def xgb_gain_importance_table(xgb_model, X, top_n=None):\n",
    "    \"\"\"\n",
    "    xgb_model: fitted xgb.XGBClassifier\n",
    "    X: training dataframe (only used for column names/order)\n",
    "    \"\"\"\n",
    "    booster = xgb_model.get_booster()\n",
    "    score_dict = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "    feature_names = list(X.columns)\n",
    "\n",
    "    rows = []\n",
    "    for k, v in score_dict.items():\n",
    "        # keys might be \"f0\", \"f1\", ... OR actual feature names\n",
    "        if k.startswith(\"f\") and k[1:].isdigit():\n",
    "            idx = int(k[1:])\n",
    "            feat = feature_names[idx] if idx < len(feature_names) else k\n",
    "        else:\n",
    "            feat = k\n",
    "        rows.append((feat, v))\n",
    "\n",
    "    imp_df = (\n",
    "        pd.DataFrame(rows, columns=[\"Feature\", \"Gain\"])\n",
    "          .sort_values(by=\"Gain\", ascending=False)\n",
    "    )\n",
    "\n",
    "    if top_n is not None:\n",
    "        imp_df = imp_df.head(top_n)\n",
    "\n",
    "    print(\"XGBoost Feature Importance (gain)\\n\")\n",
    "    print(imp_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "xgb_gain_importance_table(xgb_hyper, X_train_scaled, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2355cf6",
   "metadata": {},
   "source": [
    "## 3. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6056c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = {\n",
    "    'Algorithm': [\n",
    "        'Logistic Regression', 'Logistic Regression', 'Logistic Regression', 'Logistic Regression',\n",
    "        'Random Forest', 'Random Forest', 'Random Forest', 'Random Forest',\n",
    "        'XGBoost', 'XGBoost', 'XGBoost', 'XGBoost'\n",
    "    ],\n",
    "    'Stage': [\n",
    "        'Baseline', 'Balanced', 'Hypertuned', 'Threshold',\n",
    "        'Baseline', 'Balanced', 'Hypertuned', 'Threshold',\n",
    "        'Baseline', 'Balanced', 'Hypertuned', 'Threshold'\n",
    "    ],\n",
    "    'Threshold': [\n",
    "        0.50, 0.50, 0.50, 0.278,\n",
    "        0.50, 0.50, 0.50, 0.291,\n",
    "        0.50, 0.50, 0.50, 0.438\n",
    "    ],\n",
    "    'AUC': [\n",
    "        0.9135, 0.9160, 0.9076, 0.9076,\n",
    "        0.9140, 0.9149, 0.9247, 0.9247,\n",
    "        0.9226, 0.9285, 0.9288, 0.9288\n",
    "    ],\n",
    "    'Recall': [\n",
    "        0.57, 0.80, 0.78, 0.81,\n",
    "        0.60, 0.57, 0.73, 0.87,\n",
    "        0.58, 0.82, 0.82, 0.86\n",
    "    ],\n",
    "    'Precision': [\n",
    "        0.70, 0.53, 0.55, 0.52,\n",
    "        0.70, 0.71, 0.60, 0.51,\n",
    "        0.70, 0.53, 0.52, 0.50\n",
    "    ],\n",
    "    'F1': [\n",
    "        0.63, 0.64, 0.64, 0.63,\n",
    "        0.65, 0.63, 0.66, 0.64,\n",
    "        0.63, 0.64, 0.64, 0.64\n",
    "    ],\n",
    "    'F2': [\n",
    "        0.5889, 0.7272, 0.7210, 0.7255,\n",
    "        0.6225, 0.5948, 0.7019, 0.7596,\n",
    "        0.6010, 0.7406, 0.7381, 0.7560\n",
    "    ],\n",
    "    'Buyers_Caught': [\n",
    "        324, 460, 448, 461,\n",
    "        346, 327, 420, 496,\n",
    "        332, 471, 470, 494\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd7d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stage           AUC      Recall   Precision   F2       Buyers Caught  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline        0.9135   0.57     0.70        0.5889   324/572\n",
      "Balanced        0.9160   0.80     0.53        0.7272   460/572\n",
      "Hypertuned      0.9076   0.78     0.55        0.7210   448/572\n",
      "Threshold       0.9076   0.81     0.52        0.7255   461/572\n",
      "\n",
      "Random Forest:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stage           AUC      Recall   Precision   F2       Buyers Caught  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline        0.9140   0.60     0.70        0.6225   346/572\n",
      "Balanced        0.9149   0.57     0.71        0.5948   327/572\n",
      "Hypertuned      0.9247   0.73     0.60        0.7019   420/572\n",
      "Threshold       0.9247   0.87     0.51        0.7596   496/572\n",
      "\n",
      "XGBoost:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stage           AUC      Recall   Precision   F2       Buyers Caught  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline        0.9226   0.58     0.70        0.6010   332/572\n",
      "Balanced        0.9285   0.82     0.53        0.7406   471/572\n",
      "Hypertuned      0.9288   0.82     0.52        0.7381   470/572\n",
      "Threshold       0.9288   0.86     0.50        0.7560   494/572\n"
     ]
    }
   ],
   "source": [
    "for algo in ['Logistic Regression', 'Random Forest', 'XGBoost']:\n",
    "    algo_df = results_df[results_df['Algorithm'] == algo]\n",
    "    print(f\"\\n{algo}:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Stage':<15} {'AUC':<8} {'Recall':<8} {'Precision':<11} {'F2':<8} {'Buyers Caught':<15}\")\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in algo_df.iterrows():\n",
    "        print(f\"{row['Stage']:<15} {row['AUC']:<8.4f} {row['Recall']:<8.2f} {row['Precision']:<11.2f} {row['F2']:<8.4f} {row['Buyers_Caught']:.0f}/572\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e653789",
   "metadata": {},
   "source": [
    "### **Selected Model: Random Forest (Threshold-Tuned)**\n",
    "\n",
    "1. Highest recall (0.87) – catches 496/572 buyers (86.7%)\n",
    "\n",
    "2. Best F2 score (0.7596) – optimizes recall while maintaining acceptable precision\n",
    "\n",
    "3. Best ROI even with relatively low precision (0.51)\n",
    "    - Assume that the false postives in this model are advertisements/discounts sent to non-buyers\n",
    "    - The revenue gained from the selected model is optimized with the highest recall which outweighs the trivial cost of advertising in the e-commerce world\n",
    "    - What we truly want to optimize is the recall which represents how many potential buyers are reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62e2ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model results saved to model_comparison_results.csv in 'reports' folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('../reports', exist_ok=True)\n",
    "results_df.to_csv('../reports/model_comparison_results.csv', index=False)\n",
    "print(\"\\nModel results saved to model_comparison_results.csv in 'reports' folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecommerce-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
